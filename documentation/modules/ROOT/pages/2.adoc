== 2. Desplegar el Modelo
include::_attributes.adoc[]

[#21]
=== 2.1 ¿Qué es el modelo?

En este taller, cuando hablamos del modelo nos referimos a un **modelo de IA generativa**. Se trata de un tipo de inteligencia artificial diseñado para **crear contenido nuevo** en lugar de simplemente analizar o clasificar datos existentes.

**¿Cómo lo hacen?** Estos modelos funcionan mediante **probabilidades estadísticas**.

1. El modelo pasa por una fase de **entrenamiento**, en la que obtiene información de millones de ejemplos (textos, imágenes, código). Con estos, busca patrones para aprender las estructuras subyacentes. Así al generar una frase, calculará cuál es la siguiente palabra más probable.
2. Ya entrenado, el modelo necesita un **motor de inferencia**. El motor se encarga de ejecutar la lógica del modelo y así poder responder a los prompts del usuario.

[#22]
=== 2.2 Despliegues de OpenShift AI

En nuestro caso, vamos a desplegar un **modelo Llama 3.1** con 8 billones de parámetros en el motor de inferencia de Red Hat, el **Red Hat AI Inference Server**, que está basado en el proyecto open-source vLLM.

Vamos a volver a la pestaña de OpenShift AI. En la barra lateral, ve al **AI Hub** a la sección **Catalog**.

Aquí podrás ver una lista de modelos de IA generativa que han sido **validados** en varias configuraciones de hardware por el equipo de Red Hat y que están listos para ser desplegados en el clúster. Entre ellos, encontrarás una gran variedad de modelos, incluyendo Granite, Llama, Deepseek, Mistral, etc. 

En nuestro caso, vamos a descender hasta la sección de **Red Hat AI validated models** y clicar en **Llama-3.1-8B-Instruct**, debería ser el tercero en aparecer.

image::models.png[]

Al entrar en la página del modelo, vemos información como detalles sobe su arquitectura y su fecha de lanzamiento. Si descendemos, veremos los resultados que ha obtenido en los benchmarks más populares.

Vamos a continuar clicando el botón `Deploy model` en la esquina superior derecha.

image::model-data.png[]

Entraremos en el primer paso del formulario para configurar el despliegue del modelo. Continúa clicando en `Next`.

image::deploy-model.png[]

**MODIFICAR SEGÚN SE NECESITE** En el segundo paso, vamos especificar el proyecto, el nombre del modelo a desplegar y su perfil de hardware. Cuando esté listo, clica en `Next`.

En el tercer paso tenemos opciones de configuración avanzada. Modifica el formulario como se muestra en la siguiente imagen.

image::deploy-model-3.png[]

Llegamos al cuarto y último paso. Veremos un resumen de nuestro formulario, vamos a lanzar el despliegue clicando en `Deploy model`.

image::deploy-model-4.png[]

Al hacerlo, seremos redirigidos a la página de Deployments, donde veremos los modelos que hemos desplegado. El modelo necesitará unos minutos en desplegarse, su status debería cambiar a `Running`.

¡Hemos desplegado el modelo! Vamos a testear sus respuestas en un **playground**, un lugar donde chatear rápidamente con nuestros modelos.

Clica en la barra lateral en `Gen AI studio` en la sección `AI asset endpoints`.

image::deployments.png[]

Cambia el proyecto en la parte superior. Deberías poder ver el modelo que acabas de desplegar. Continua pinchando en `Add to playground` y luego `Create`.

image::assets.png[]

Puede que al hacerlo, salte el siguiente mensaje de error. Estos espacios de testeo aún están en Tech Preview. Puedes ignorarlo, cerrarlo pinchando en la `X` y continuar con el siguiente paso sin problema.

image::playground-error.png[]

Vamos a acceder al espacio de pruebas clicando en la barra lateral izquierda en `Gen AI studio` en la sección `Playground`.

image::playground-chat.png[]

¡Perfecto! Prueba a escribir algo en la barra inferior para que el modelo responda.

Hemos terminado esta parte. Pasa a la sección **3.Desarrollar el Servidor MCP**.

